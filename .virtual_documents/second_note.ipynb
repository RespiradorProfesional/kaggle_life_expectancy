


import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.feature_selection import SelectKBest, f_classif
import numpy as np
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split







df = pd.read_csv('train.csv')

pd.set_option('display.max_columns', None)

df


df.describe()


df.info()


df.duplicated().value_counts()


df['Status'].unique()


df['Country'] = df['Country'].str.capitalize()



le=LabelEncoder()

df['Status']= le.fit_transform(df['Status'])
print(df['Status'].head(5)) #1 devoloping 0 devoloped

df['Country_encoded'] = le.fit_transform(df['Country'])
print(df['Country_encoded'].head(5)) 


print(df.columns)

df.columns = df.columns.map(lambda x: x.strip())
print(df.columns)


df.max()





plt.figure(figsize=(12,6))
sns.histplot(df['Life expectancy'], edgecolor='black')
plt.show()


plt.scatter(df['Life expectancy'] , df['Alcohol'], color='blue')  
plt.show()


plt.scatter(df['Life expectancy'] , df['BMI'], color='blue')  
plt.xlabel('Life expectancy')
plt.ylabel('BMI')
plt.show()


sns.barplot(x="Status", y='Life expectancy', data=df)
plt.show()





# Correlation matrix

# Seleccionar solo las columnas numéricas
numeric_columns = df.select_dtypes(include=['number'])

correlation_matr= numeric_columns.corr().round(1)

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matr, annot=True, cmap='coolwarm', center=0)
plt.title('Correlation Matrix')
plt.show()


X_df = df.drop(columns=['Life expectancy', 'Country'])
y_df = df['Life expectancy']


# Filter Method for Feature Selection
selector = SelectKBest(score_func=f_classif, k=5)
selected_features = selector.fit_transform(X_df, y_df)

print("Selected Features Shape:", selected_features.shape)
selected_feature_names = X_df.columns[selector.get_support()] 

selected_df = df[selected_feature_names].copy()
selected_df['target']=df['Life expectancy']


# Correlation matrix

correlation_matr= selected_df.corr().round(2)

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matr, annot=True, cmap='coolwarm', center=0)
plt.title('Correlation Matrix')
plt.show()


# Calculating VIF (Variance Inflation Factor) to check for multicollinearity


X_2 = selected_df.drop(columns=['target','Schooling'])
y = selected_df['target']

# Calculate VIF for each feature
vif_data = pd.DataFrame()
vif_data['Feature'] = X_2.columns
vif_data['VIF'] = [variance_inflation_factor(X_2.values, i) for i in range(X_2.shape[1])]

print(vif_data)





#X_2 have the correct columns

X_train, X_test, y_train, y_test = train_test_split(X_2, y_df, test_size=0.2, random_state=42)


from sklearn.linear_model import Lasso, Ridge, LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score

import numpy as np

# Crear un Pipeline con un paso genérico para el modelo
pipeline = Pipeline([
    ('poly', PolynomialFeatures()),  # Para la regresión polinómica
    ('model', LinearRegression())  # Este será sustituido en el GridSearch
])

# Definir los hiperparámetros para cada modelo
param_grid = [
    # Regresión Lineal con diferentes grados
    {
        'poly__degree': [1, 2, 3, 4],  # Grados para la regresión polinómica
        'model': [LinearRegression()]
    },
    # Ridge Regression
    {
        'poly__degree': [1],  # No aplicar polinomio
        'model': [Ridge()],
        'model__alpha': [0.1, 1.0, 10.0]  # Regularización Ridge
    },
    # Lasso Regression
    {
        'poly__degree': [1],  # No aplicar polinomio
        'model': [Lasso()],
        'model__alpha': [0.1, 1.0, 10.0]  # Regularización Lasso
    }
]

# Configurar el GridSearchCV
grid_search = GridSearchCV(
    estimator=pipeline,
    param_grid=param_grid,
    cv=5,  # Validación cruzada
    scoring='neg_mean_squared_error',  # Usamos el error cuadrático medio
    verbose=2,
    n_jobs=-1  # Usar todos los núcleos disponibles
)

# Ejecutar el GridSearch
grid_search.fit(X_train, y_train)

# Mostrar los mejores hiperparámetros y el modelo ganador
print("Best Parameters:", grid_search.best_params_)
print("Best Score (Negative MSE):", grid_search.best_score_)

# Predecir con el mejor modelo
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

# Calcular el error cuadrático medio
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

r2 = r2_score(y_test, y_pred)
print(f'R²: {r2}')

# Mostrar el modelo final seleccionado
print("Best Model:", best_model)





df_test = pd.read_csv('test.csv')

df_test['Country'] = df_test['Country'].str.capitalize()

le=LabelEncoder()

df_test['Status']= le.fit_transform(df_test['Status'])

df_test['Country_encoded'] = le.fit_transform(df_test['Country'])

df_test.columns = df_test.columns.map(lambda x: x.strip())

df_test= df_test.drop(columns=[col for col in df_test.columns if col not in X_2.columns])





y_pred = best_model.predict(df_test)


predict_csv = pd.DataFrame()
predict_csv['ID']= 0
predict_csv['Life expectancy']= y_pred
predict_csv['ID']= predict_csv.index+1
predict_csv


predict_csv.to_csv('predict.csv', index=False)
