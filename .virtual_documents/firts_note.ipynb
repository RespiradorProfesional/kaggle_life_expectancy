


import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.feature_selection import SelectKBest, f_regression
import numpy as np
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline






df = pd.read_csv('train.csv')

pd.set_option('display.max_columns', None)

df


df.describe()


df.info()


df.duplicated().value_counts()


df['Status'].unique()


df['Country'] = df['Country'].str.capitalize()



le=LabelEncoder()

df['Status']= le.fit_transform(df['Status'])
print(df['Status'].head(5)) #1 devoloping 0 devoloped

df['Country_encoded'] = le.fit_transform(df['Country'])
print(df['Country_encoded'].head(5)) 


print(df.columns)

df.columns = df.columns.map(lambda x: x.strip())

print(df.columns)


df.max()


#La columna de "infant deaths" no tiene sentido ya que es cada 1000 habitantes el numero de niños, pero lo dejare asi ya que no es necesaria

# Adulto mortality tiene 1510 columnas que superan el 100 suponiendo que este deberia ser un porcentaje
#aunque puede ser que estos numeros sean los muertos en cada 1000 habitantes
print(df['Adult Mortality'][df['Adult Mortality']>100].count())
print(df['Adult Mortality'][df['Adult Mortality']>1000].count())

# BMI mayores de 45 ya ES MUY RARO asi que puse como maximo 50
print(df['BMI'][df['BMI']>50].count())
df['BMI'] = df['BMI'].clip(lower=5, upper=50)

# Es muy raro que percentage expenditure supere el 100% en 1040 valores de 2350
print(df['percentage expenditure'][df['percentage expenditure']>70].count())
df=df.drop(columns=['percentage expenditure'])




continuous_features = ['Year', 'Adult Mortality', 'infant deaths', 'Alcohol', 
                       'Hepatitis B', 'Measles', 'BMI', 'under-five deaths', 
                       'Polio', 'Total expenditure', 'Diphtheria', 'HIV/AIDS', 
                       'GDP', 'Population', 'thinness  1-19 years', 
                       'thinness 5-9 years', 'Income composition of resources', 
                       'Schooling']

# Crear el escalador (StandardScaler o MinMaxScaler)
scaler = StandardScaler()  # O usa MinMaxScaler()

print(df.columns)
# Normalizar las columnas seleccionadas
df[continuous_features] = scaler.fit_transform(df[continuous_features])

df






plt.figure(figsize=(12,6))
sns.histplot(df['Life expectancy'], edgecolor='black')
plt.show()


plt.scatter(df['Life expectancy'] , df['Alcohol'], color='blue')  
plt.show()


plt.scatter(df['Life expectancy'] , df['BMI'], color='blue')  
plt.xlabel('Life expectancy')
plt.ylabel('BMI')
plt.show()


sns.barplot(x="Status", y='Life expectancy', data=df)
plt.show()





# Correlation matrix

# Seleccionar solo las columnas numéricas
numeric_columns = df.select_dtypes(include=['number'])

correlation_matr= numeric_columns.corr().round(1)

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matr, annot=True, cmap='coolwarm', center=0)
plt.title('Correlation Matrix')
plt.show()


X_df = df.drop(columns=['Life expectancy', 'Country'])
y_df = df['Life expectancy']


# Seleccion de features

selector = SelectKBest(score_func=f_regression, k=7)
selected_features = selector.fit_transform(X_df, y_df)

print("Selected Features Shape:", selected_features.shape)
selected_feature_names = X_df.columns[selector.get_support()] 

selected_df = X_df[selected_feature_names].copy()

selected_df['target']=df['Life expectancy']


# Correlation matrix de los seleccionados

correlation_matr= selected_df.corr().round(2)

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matr, annot=True, cmap='coolwarm', center=0)
plt.title('Correlation Matrix')
plt.show()



# Calculo del VIF

X = selected_df.drop(columns=['target','Schooling'])
y = selected_df['target']

vif_data = pd.DataFrame()
vif_data['Feature'] = X.columns
vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

print(vif_data)





X_train, X_test, y_train, y_test = train_test_split(X, y_df, test_size=0.2, random_state=42)


from sklearn.linear_model import Lasso, Ridge, LinearRegression
from sklearn.svm import SVR  # Importar el modelo SVR
from sklearn.ensemble import GradientBoostingRegressor  # Importar Gradient Boosting
from sklearn.ensemble import RandomForestRegressor  # Corregido el modelo RandomForestRegressor
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score

import numpy as np

# Crear un Pipeline con un paso genérico para el modelo
pipeline = Pipeline([
    ('poly', PolynomialFeatures()),  # Para la regresión polinómica
    ('model', LinearRegression())  # Este será sustituido en el GridSearch
])

# Definir los hiperparámetros para cada modelo
param_grid = [
    # Regresión Lineal con diferentes grados
    {
        'poly__degree': [1, 2, 3, 4],  # Grados para la regresión polinómica
        'model': [LinearRegression()]
    },
    # Ridge Regression
    {
        'poly__degree': [1],  # No aplicar polinomio
        'model': [Ridge()],
        'model__alpha': [0.1, 1.0, 10.0]  # Regularización Ridge
    },
    # Lasso Regression
    {
        'poly__degree': [1],  # No aplicar polinomio
        'model': [Lasso()],
        'model__alpha': [0.1, 1.0, 10.0]  # Regularización Lasso
    },
    # RandomForest Regressor
    {
        'poly__degree': [1],  # No aplicar polinomio
        'model': [RandomForestRegressor()],
        'model__n_estimators': [100, 200],  # Número de árboles
        'model__max_depth': [None, 10, 20]  # Profundidad máxima de los árboles
    }
]

# Configurar el GridSearchCV
grid_search = GridSearchCV(
    estimator=pipeline,
    param_grid=param_grid,
    cv=5,  # Validación cruzada
    scoring='neg_mean_squared_error',  # Usamos el error cuadrático medio
    verbose=2,
    n_jobs=-1  # Usar todos los núcleos disponibles
)

# Ejecutar el GridSearch
grid_search.fit(X_train, y_train)

# Mostrar los mejores hiperparámetros y el modelo ganador
print("Best Parameters:", grid_search.best_params_)
print("Best Score (Negative MSE):", grid_search.best_score_)

# Predecir con el mejor modelo
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

# Calcular el error cuadrático medio
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

r2 = r2_score(y_test, y_pred)
print(f'R²: {r2}')

# Mostrar el modelo final seleccionado
print("Best Model:", best_model)





df_test = pd.read_csv('test.csv')

df_test['Country'] = df_test['Country'].str.capitalize()

le=LabelEncoder()

df_test['Status']= le.fit_transform(df_test['Status'])

df_test['Country_encoded'] = le.fit_transform(df_test['Country'])

df_test.columns = df_test.columns.map(lambda x: x.strip())

df_test['BMI'] = df_test['BMI'].clip(lower=5, upper=50)

df_test=df_test.drop(columns=['percentage expenditure'])


continuous_features = ['Year', 'Adult Mortality', 'infant deaths', 'Alcohol', 
                       'Hepatitis B', 'Measles', 'BMI', 'under-five deaths', 
                       'Polio', 'Total expenditure', 'Diphtheria', 'HIV/AIDS', 
                       'GDP', 'Population', 'thinness  1-19 years', 
                       'thinness 5-9 years', 'Income composition of resources', 
                       'Schooling']


scaler = StandardScaler()  

df_test[continuous_features] = scaler.fit_transform(df_test[continuous_features])



df_test = df_test[[col for col in X.columns if col in df_test.columns]]





y_pred = best_model.predict(df_test)


predict_csv = pd.DataFrame()
predict_csv['ID']= 0
predict_csv['Life expectancy']= y_pred
predict_csv['ID']= predict_csv.index+1
predict_csv


predict_csv.to_csv('predict.csv', index=False)






